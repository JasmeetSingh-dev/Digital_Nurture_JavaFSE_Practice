Big O Notation is a mathematical way to describe how the runtime or space requirements of an algorithm grow as the input size increases.

It helps us to predict performance and compare efficiency of algorithms